{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPM7JcpqNdCv2hlaLJrYCQO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5. 인공신경망"],"metadata":{"id":"iTN3wLfljxYh"}},{"cell_type":"markdown","source":["## 5.4 최적화 기법"],"metadata":{"id":"HtuRCWGDkJ0u"}},{"cell_type":"code","source":["import torch\n","import torchvision"],"metadata":{"id":"WpzNBLwykPK6","executionInfo":{"status":"ok","timestamp":1691281329980,"user_tz":-540,"elapsed":6141,"user":{"displayName":"이기환","userId":"15706483123137731910"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["model = torchvision.models.resnet18(pretrained=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZEMDs1RkRoD","executionInfo":{"status":"ok","timestamp":1691281330694,"user_tz":-540,"elapsed":722,"user":{"displayName":"이기환","userId":"15706483123137731910"}},"outputId":"94a2512c-cd5c-490a-ddcb-d66e8a79b35f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"markdown","source":["### 5.4.1 SGD"],"metadata":{"id":"WUy25h4WkWgs"}},{"cell_type":"code","source":["# 한 번 학습 시 전체 데이터를 사용하는 것이 아닌 일부 데이터를 사용하는 gradient descent 방법\n","# 하지만 lr이 고정되어 있다. => Momentum base / Adam\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"kQJd_l_EkY_U","executionInfo":{"status":"ok","timestamp":1691281330695,"user_tz":-540,"elapsed":9,"user":{"displayName":"이기환","userId":"15706483123137731910"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 5.4.2 Momentum & Adam"],"metadata":{"id":"w05ll1x6kefO"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"],"metadata":{"id":"OKmiWSNmkhlt","executionInfo":{"status":"ok","timestamp":1691281330696,"user_tz":-540,"elapsed":9,"user":{"displayName":"이기환","userId":"15706483123137731910"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"],"metadata":{"id":"21gxS0nfk4gH","executionInfo":{"status":"ok","timestamp":1691281330696,"user_tz":-540,"elapsed":9,"user":{"displayName":"이기환","userId":"15706483123137731910"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 5.4.3 스케줄링 : lr을 바꿔주는 것\n","* Adam을 사용할 때는 내부에서 lr이 바뀌어서 굳이 스케줄링을 사용할 필요가 없다."],"metadata":{"id":"j12w5hb7k6wP"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","schduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","# 일정 스텝 마다 학습률을 줄이거나 늘릴 수 있는 것\n","# step_sszie=30이면 에톡이 30번 돌 때 마다 감마 0.1을 학습률에 곱해서 사용한다.\n","# 즉, 0.1 / 30 이후 0.01 / 60 이후 0.001\n","# Cos을 이용해서 올라갔다 내려갔다 할 수 있게 -> local minimum에 빠질 위험을 방지"],"metadata":{"id":"GNQ7qzWIk9FF","executionInfo":{"status":"ok","timestamp":1691281568785,"user_tz":-540,"elapsed":366,"user":{"displayName":"이기환","userId":"15706483123137731910"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["... 중략 ...\n","\n","for epoch in range(400):\n","  running_loss = 0.0\n","  for data in trainloader:\n","    inputs, values = data\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, values)\n","    loss.backward()\n","    optimizer.step()\n","\n","    ... 중략 ,,,\n","\n","scheduler.step() # 스케줄링ㅇ을 통한 학습률 조정"],"metadata":{"id":"f-H1o5XkPHvX"},"execution_count":null,"outputs":[]}]}